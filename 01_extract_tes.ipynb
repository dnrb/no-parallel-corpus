{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f19f431-4845-4eb4-b4b6-d1c0c149c7ea",
   "metadata": {},
   "source": [
    "## libraries and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b24b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# #\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.sparse import csr_matrix\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from unidecode import unidecode\n",
    "from editdistance import eval as ed\n",
    "from itertools import chain, combinations\n",
    "from multiprocessing import Pool\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11d91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BITEXT FUNCTIONS\n",
    "good_pos = {'NOUN','ADJ','VERB'}\n",
    "excluded_lemmas = {'be', 'other', 'have', 'let', 'one', 'lot', 'same', 'such', 't', 's'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aaf3a9-d951-46f5-9631-20dd36db3666",
   "metadata": {},
   "source": [
    "## Liu et al reimplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5803ab59-dbe8-43c8-82bd-d178a13dd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fragment extraction\n",
    "\n",
    "def get_fragments(w, min_len=2, max_len=7, yield_self=True):\n",
    "    \"\"\"\n",
    "    takes a string *w* and extracts all substrings of minimal length *min_len* and maximal length *max_len*\n",
    "    \"\"\"\n",
    "    if yield_self and len(w) > max_len: yield w\n",
    "    for i in range(len(w)):\n",
    "        for j in range(i+min_len, min(i+max_len+1,len(w)+1)):\n",
    "            if (not (i == 0 or j == len(w))) or (j-i) >= min_len+1:\n",
    "                yield w[i:j]\n",
    "\n",
    "def get_all_fragments(F, split_words = True, frequency_threshold = 1):\n",
    "    \"\"\"\n",
    "    given a document stored in a dictionary *tbd*, mapping an identifier key \n",
    "    onto a string containing the text, and a set of *all_verses*\n",
    "    (the shared identifier keys between tbd and the source document(,\n",
    "    this function returns a sparse matrix *fragments* of identifier key (rows) \n",
    "    to substrings of the text (column), with the matrix being True if the fragment\n",
    "    occurs for that identifier key and False otherwise.\n",
    "    as well as dictionaries for the identification\n",
    "    of the rows and columns.\n",
    "    (memory/computation efficient format, but a bit densely written)\n",
    "    \"\"\"\n",
    "    wordcount = Counter((w for l in F for w in l))\n",
    "    if split_words: \n",
    "        word_fragments = {w : set(get_fragments('^%s$' % unidecode(w).lower())) for w in wordcount.keys() }\n",
    "    else: \n",
    "        word_fragments = {w : {unidecode(w).lower()} for w in wordcount.keys() }\n",
    "    \n",
    "    fragment_count = Counter((f for w,F in word_fragments.items() for f in F if f != '' for i in range(wordcount[w])))\n",
    "    fragment_ixx = {f:i for i,(f,c) in enumerate(fragment_count.most_common()) if c >= frequency_threshold }\n",
    "    #\n",
    "    R,C = [], []\n",
    "    for line_ix, line_f in enumerate(F):\n",
    "        if len(line_f) == 0: continue\n",
    "        line_frags = list(map(lambda f : fragment_ixx[f],\n",
    "                              filter(lambda f : f in fragment_ixx,\n",
    "                                     set.union(*map(lambda w : word_fragments[w], line_f)))))\n",
    "        R.extend([line_ix]*len(line_frags))\n",
    "        C.extend(line_frags)\n",
    "    fragments = csr_matrix((np.ones(len(R)), (R,C)), dtype=bool, shape = (len(F),max(C)+1))\n",
    "    return fragments, fragment_ixx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88cde75f-2e7f-44ae-a456-5870c762f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alignments(doc, dataset, min_freq = 1, verbose=False):\n",
    "    #\n",
    "    E,F = zip(*[([w.split('/')[2] for w in l.strip('\\n').split(' ||| ')[1].split() \n",
    "                  if w.count('/') >= 3 and w.split('/')[3] in good_pos and w.split('/')[2] not in excluded_lemmas],\n",
    "                  l.strip('\\n').split(' ||| ')[0].split())\n",
    "                 for l in open('./generated/%s_bitexts/%s.spc' % (dataset,doc))])\n",
    "    e_fragments, e_dic = get_all_fragments(E, split_words=False, frequency_threshold=1)\n",
    "    e_counts = Counter([unidecode(e).lower() for l in E for e in l if e != ''])\n",
    "    e_seed = {e : e_dic[e] for e in e_counts if e_counts[e] >= min_freq and e != ''}\n",
    "    f_fragments, f_dic = get_all_fragments(F, split_words=True, frequency_threshold=min_freq)\n",
    "    f_list = np.array(sorted(f_dic, key = lambda k : f_dic[k]))\n",
    "    #\n",
    "    # get TEs\n",
    "    tes, te_words = {}, {}\n",
    "    for erank,(e,ei) in enumerate(sorted(e_seed.items(), key = lambda x : -e_counts[x[0]])):\n",
    "        pos = e_fragments[:,ei].nonzero()[0]\n",
    "        tes[e] = extract_tes(pos, f_fragments, f_dic, coverage=.95, min_trans=0.01, min_backtrans=0.10)\n",
    "        #tes[e] = merge_similar_tes(tes[e])\n",
    "        te_words[e] = { te : get_te_words(f_fragments, f_list, te, te_pos) for te,te_pos in tes[e].items() }\n",
    "        if verbose: print(doc, e, '%d/%d' % (erank,len(e_seed)), e_counts[e], datetime.now(), \n",
    "                          Counter({t:len(p) for t,p in tes[e].items()}).most_common(3))\n",
    "    print(doc, datetime.now(), len(tes))\n",
    "    with open('./generated/%s_output/%s.json' % (dataset,doc),'w') as fout:\n",
    "        fout.write(json.dumps({'tes' : tes, 'te_words' : te_words}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e956fbf-8007-4a40-b0e5-cdaf90f12bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_nonoverlapping(m, rev, frags):\n",
    "    frags = [rev[f] for f in frags]\n",
    "    Ma = [mi for mi in frags if m in mi and \n",
    "          next((False for mj in frags if mj != mi and mi in mj),True)]\n",
    "    return Ma\n",
    "\n",
    "def get_te_words(fragments_F, rev, te, te_pos):\n",
    "    ctr = Counter()\n",
    "    for pos in te_pos:\n",
    "        frags = fragments_F[pos].nonzero()[1]\n",
    "        Ma = longest_nonoverlapping(te, rev, frags)\n",
    "        for m in Ma:\n",
    "            ctr[m] += 1\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77243a6-433a-44ac-91bd-d052492e5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tes(pos_ixx, fragments, fixx, coverage = 0.95, min_trans = 0.05, min_backtrans = 0.25, verbose=False):\n",
    "    \"\"\"\n",
    "    implements one forward pass of the Liu et al. (2023) approach.\n",
    "    Takes a list *pos* of positive instances (row identifiers for fragments)\n",
    "    As well as the sparse matrix *fragments* and the two dictionaries\n",
    "    (vixx -- verse_ixx and fixx -- fragment_ixx).\n",
    "    The parameters determine the fragments considered in the extraction: the iteration keeps going until\n",
    "    either no good fragments can be found (significance under .001) or the *coverage* has been reached.\n",
    "    *min_trans* is a float [0,1] that filters out all fragments that occur in hit verses in less than min_trans\n",
    "    proportion of all hit verses.\n",
    "    \"\"\"\n",
    "    neg_ixx = list(set(range(fragments.shape[0]))-set(pos_ixx))\n",
    "    flist = [None] * len(fixx)\n",
    "    for k,v in fixx.items():\n",
    "        flist[v] = k\n",
    "    #sorted(fixx, key = lambda k : fixx[k])\n",
    "    #\n",
    "    pos_tot_orig = pos_tot = len(pos_ixx)\n",
    "    neg_tot = len(neg_ixx)\n",
    "    #\n",
    "    pos_ct = fragments[pos_ixx].sum(0).A[0]\n",
    "    neg_ct = fragments[neg_ixx].sum(0).A[0]\n",
    "    #\n",
    "    good_fragments = np.where((pos_ct >= 1) & ((pos_ct/pos_tot_orig) >= min_trans) &\n",
    "                             (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
    "    string_props = [(f[0] == '^', f[-1] == '$', len(f)) for f in flist]\n",
    "                    #len(re.sub('[.*]', '', f)))\n",
    "    #\n",
    "    hits = defaultdict(lambda : [])\n",
    "    ct = 0\n",
    "    fe_scores = {}\n",
    "    while len(pos_ixx) >= (1-coverage) * pos_tot_orig:\n",
    "        ct += 1\n",
    "        #\n",
    "        # GET BEST\n",
    "        assoc_scores = Counter()\n",
    "        for f in good_fragments:\n",
    "            table = ((pos_ct[f],pos_tot-pos_ct[f]),(neg_ct[f],neg_tot-neg_ct[f]))\n",
    "            try: fe_score = fe_scores[table]\n",
    "            except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n",
    "            assoc_scores[f] = (fe_score, string_props[f])\n",
    "        best, best_score = next((x for x in assoc_scores.most_common(1)),(None,None))\n",
    "        if best == None or best_score[0] < -np.log(5e-2):\n",
    "            break\n",
    "        # print([flist[k] for k,v in assoc_scores.most_common(10)])\n",
    "        #\n",
    "        # UPDATE\n",
    "        new_pos_ixx = []\n",
    "        for pos_v in pos_ixx:\n",
    "            if fragments[pos_v,best] > 0: hits[flist[best]].append(int(pos_v))\n",
    "            else: new_pos_ixx.append(pos_v)\n",
    "        #\n",
    "        pos_ixx = new_pos_ixx\n",
    "        pos_tot = len(pos_ixx)\n",
    "        pos_ct = fragments[pos_ixx].sum(0).A[0]\n",
    "        neg_ct = fragments[neg_ixx].sum(0).A[0]\n",
    "        #\n",
    "        good_fragments = np.where((pos_ct >= 1) & (pos_ct/pos_tot_orig >= min_trans) &\n",
    "                                    (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
    "        if verbose: print(ct, flist[best])\n",
    "    return hits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505dfab-f473-4458-be42-830032aa01c5",
   "metadata": {},
   "source": [
    "## execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "200a83f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_2603394/43898795.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeha1242 2025-04-18 17:14:39.731175 514\n",
      "apah1238 2025-04-18 17:14:47.498563 670\n",
      "goem1240 2025-04-18 17:14:52.368940 989\n",
      "guri1247 2025-04-18 17:14:53.240213 724\n",
      "cabe1245 2025-04-18 17:14:57.094581 813\n",
      "jeju1234 2025-04-18 17:15:17.597729 963\n",
      "hoch1243 2025-04-18 17:15:18.821600 655\n",
      "anal1239 2025-04-18 17:15:28.313230 1189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2603394/43898795.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligh1234 2025-04-18 17:15:29.614844 459\n",
      "cash1254 2025-04-18 17:15:35.882616 976\n",
      "arap1274 2025-04-18 17:15:37.842005 1056\n",
      "kaka1265 2025-04-18 17:15:41.626992 1804\n",
      "goro1270 2025-04-18 17:15:42.465707 1115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2603394/43898795.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngal1292 2025-04-18 17:15:43.106620 491\n",
      "kark1256 2025-04-18 17:15:53.483402 887\n",
      "dolg1241 2025-04-18 17:15:54.064859 1389\n",
      "nngg1234 2025-04-18 17:16:00.591341 1054\n",
      "sanz1248 2025-04-18 17:16:07.302533 710\n",
      "sout2856 2025-04-18 17:16:23.515610 941\n",
      "savo1255 2025-04-18 17:16:27.032253 564\n",
      "ruul1235 2025-04-18 17:16:46.806399 1297\n",
      "pnar1238 2025-04-18 17:16:51.132689 1340\n",
      "sumi1235 2025-04-18 17:16:55.871911 735\n",
      "sadu1234 2025-04-18 17:17:01.953965 969\n",
      "teop1238 2025-04-18 17:17:03.136264 667\n",
      "even1259 2025-04-18 17:17:04.313404 1038\n",
      "vera1241 2025-04-18 17:17:17.646562 771\n",
      "texi1237 2025-04-18 17:17:22.043539 613\n",
      "beja1238 2025-04-18 17:17:23.210239 1317\n",
      "kama1351 2025-04-18 17:17:27.064510 1421\n",
      "svan1243 2025-04-18 17:17:28.166817 1224\n",
      "warl1254 2025-04-18 17:17:30.201208 442\n",
      "komn1238 2025-04-18 17:17:38.227544 1503\n",
      "taba1259 2025-04-18 17:17:38.712080 583\n",
      "trin1278 2025-04-18 17:18:01.342957 1420\n",
      "urum1249 2025-04-18 17:18:36.065223 976\n"
     ]
    }
   ],
   "source": [
    "# DoReCo\n",
    "with Pool(8) as p:\n",
    "    p.starmap(create_alignments, map(lambda doc : (doc.strip('\\n'), 'doreco', 1), open('./doreco_doculects.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5036f426-a05b-4871-918f-ebcdbd3e7c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OPUS\n",
    "with Pool(4) as p:\n",
    "    p.starmap(create_alignments, map(lambda doc : (doc.strip('\\n'), 'opus', 30, True), ['es','fi', 'tr', 'el']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
