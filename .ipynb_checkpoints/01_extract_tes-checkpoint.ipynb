{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f19f431-4845-4eb4-b4b6-d1c0c149c7ea",
   "metadata": {},
   "source": [
    "## libraries and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b24b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "# #\n",
    "from scipy.stats import fisher_exact\n",
    "from scipy.sparse import csr_matrix\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from unidecode import unidecode\n",
    "from editdistance import eval as ed\n",
    "from itertools import chain, combinations\n",
    "from multiprocessing import Pool\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11d91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BITEXT FUNCTIONS\n",
    "good_pos = {'NOUN','ADJ','VERB'}\n",
    "excluded_lemmas = {'be', 'other', 'have', 'let', 'one', 'lot', 'same', 'such', 't', 's'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aaf3a9-d951-46f5-9631-20dd36db3666",
   "metadata": {},
   "source": [
    "## Liu et al reimplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5803ab59-dbe8-43c8-82bd-d178a13dd47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fragment extraction\n",
    "\n",
    "def get_fragments(w, min_len=2, max_len=7, yield_self=True):\n",
    "    \"\"\"\n",
    "    takes a string *w* and extracts all substrings of minimal length *min_len* and maximal length *max_len*\n",
    "    \"\"\"\n",
    "    if yield_self and len(w) > max_len: yield w\n",
    "    for i in range(len(w)):\n",
    "        for j in range(i+min_len, min(i+max_len+1,len(w)+1)):\n",
    "            if (not (i == 0 or j == len(w))) or (j-i) >= min_len+1:\n",
    "                yield w[i:j]\n",
    "\n",
    "def get_all_fragments(F, split_words = True, frequency_threshold = 1):\n",
    "    \"\"\"\n",
    "    given a document stored in a dictionary *tbd*, mapping an identifier key \n",
    "    onto a string containing the text, and a set of *all_verses*\n",
    "    (the shared identifier keys between tbd and the source document(,\n",
    "    this function returns a sparse matrix *fragments* of identifier key (rows) \n",
    "    to substrings of the text (column), with the matrix being True if the fragment\n",
    "    occurs for that identifier key and False otherwise.\n",
    "    as well as dictionaries for the identification\n",
    "    of the rows and columns.\n",
    "    (memory/computation efficient format, but a bit densely written)\n",
    "    \"\"\"\n",
    "    wordcount = Counter((w for l in F for w in l))\n",
    "    if split_words: \n",
    "        word_fragments = {w : set(get_fragments('^%s$' % unidecode(w).lower())) for w in wordcount.keys() }\n",
    "    else: \n",
    "        word_fragments = {w : {unidecode(w).lower()} for w in wordcount.keys() }\n",
    "    \n",
    "    fragment_count = Counter((f for w,F in word_fragments.items() for f in F if f != '' for i in range(wordcount[w])))\n",
    "    fragment_ixx = {f:i for i,(f,c) in enumerate(fragment_count.most_common()) if c >= frequency_threshold }\n",
    "    #\n",
    "    R,C = [], []\n",
    "    for line_ix, line_f in enumerate(F):\n",
    "        if len(line_f) == 0: continue\n",
    "        line_frags = list(map(lambda f : fragment_ixx[f],\n",
    "                              filter(lambda f : f in fragment_ixx,\n",
    "                                     set.union(*map(lambda w : word_fragments[w], line_f)))))\n",
    "        R.extend([line_ix]*len(line_frags))\n",
    "        C.extend(line_frags)\n",
    "    fragments = csr_matrix((np.ones(len(R)), (R,C)), dtype=bool, shape = (len(F),max(C)+1))\n",
    "    return fragments, fragment_ixx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88cde75f-2e7f-44ae-a456-5870c762f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alignments(doc, dataset, min_freq = 1, verbose=False):\n",
    "    #\n",
    "    E,F = zip(*[([w.split('/')[2] for w in l.strip('\\n').split(' ||| ')[1].split() \n",
    "                  if w.count('/') >= 3 and w.split('/')[3] in good_pos and w.split('/')[2] not in excluded_lemmas],\n",
    "                  l.strip('\\n').split(' ||| ')[0].split())\n",
    "                 for l in open('./generated/%s_bitexts/%s.spc' % (dataset,doc))])\n",
    "    e_fragments, e_dic = get_all_fragments(E, split_words=False, frequency_threshold=1)\n",
    "    e_counts = Counter([unidecode(e).lower() for l in E for e in l if e != ''])\n",
    "    e_seed = {e : e_dic[e] for e in e_counts if e_counts[e] >= min_freq and e != ''}\n",
    "    f_fragments, f_dic = get_all_fragments(F, split_words=True, frequency_threshold=min_freq)\n",
    "    f_list = np.array(sorted(f_dic, key = lambda k : f_dic[k]))\n",
    "    #\n",
    "    # get TEs\n",
    "    tes, te_words = {}, {}\n",
    "    for erank,(e,ei) in enumerate(sorted(e_seed.items(), key = lambda x : -e_counts[x[0]])):\n",
    "        pos = e_fragments[:,ei].nonzero()[0]\n",
    "        tes[e] = extract_tes(pos, f_fragments, f_dic, coverage=.95, min_trans=0.01, min_backtrans=0.10)\n",
    "        #tes[e] = merge_similar_tes(tes[e])\n",
    "        te_words[e] = { te : get_te_words(f_fragments, f_list, te, te_pos) for te,te_pos in tes[e].items() }\n",
    "        if verbose: print(doc, e, '%d/%d' % (erank,len(e_seed)), e_counts[e], datetime.now(), \n",
    "                          Counter({t:len(p) for t,p in tes[e].items()}).most_common(3))\n",
    "    print(doc, datetime.now(), len(tes))\n",
    "    with open('./generated/%s_output/%s.json' % (dataset,doc),'w') as fout:\n",
    "        fout.write(json.dumps({'tes' : tes, 'te_words' : te_words}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e956fbf-8007-4a40-b0e5-cdaf90f12bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longest_nonoverlapping(m, rev, frags):\n",
    "    frags = [rev[f] for f in frags]\n",
    "    Ma = [mi for mi in frags if m in mi and \n",
    "          next((False for mj in frags if mj != mi and mi in mj),True)]\n",
    "    return Ma\n",
    "\n",
    "def get_te_words(fragments_F, rev, te, te_pos):\n",
    "    ctr = Counter()\n",
    "    for pos in te_pos:\n",
    "        frags = fragments_F[pos].nonzero()[1]\n",
    "        Ma = longest_nonoverlapping(te, rev, frags)\n",
    "        for m in Ma:\n",
    "            ctr[m] += 1\n",
    "    return ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a77243a6-433a-44ac-91bd-d052492e5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tes(pos_ixx, fragments, fixx, coverage = 0.95, min_trans = 0.05, min_backtrans = 0.25, verbose=False):\n",
    "    \"\"\"\n",
    "    implements one forward pass of the Liu et al. (2023) approach.\n",
    "    Takes a list *pos* of positive instances (row identifiers for fragments)\n",
    "    As well as the sparse matrix *fragments* and the two dictionaries\n",
    "    (vixx -- verse_ixx and fixx -- fragment_ixx).\n",
    "    The parameters determine the fragments considered in the extraction: the iteration keeps going until\n",
    "    either no good fragments can be found (significance under .001) or the *coverage* has been reached.\n",
    "    *min_trans* is a float [0,1] that filters out all fragments that occur in hit verses in less than min_trans\n",
    "    proportion of all hit verses.\n",
    "    \"\"\"\n",
    "    neg_ixx = list(set(range(fragments.shape[0]))-set(pos_ixx))\n",
    "    flist = [None] * len(fixx)\n",
    "    for k,v in fixx.items():\n",
    "        flist[v] = k\n",
    "    #sorted(fixx, key = lambda k : fixx[k])\n",
    "    #\n",
    "    pos_tot_orig = pos_tot = len(pos_ixx)\n",
    "    neg_tot = len(neg_ixx)\n",
    "    #\n",
    "    pos_ct = fragments[pos_ixx].sum(0).A[0]\n",
    "    neg_ct = fragments[neg_ixx].sum(0).A[0]\n",
    "    #\n",
    "    good_fragments = np.where((pos_ct >= 1) & ((pos_ct/pos_tot_orig) >= min_trans) &\n",
    "                             (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
    "    string_props = [(f[0] == '^', f[-1] == '$', len(f)) for f in flist]\n",
    "                    #len(re.sub('[.*]', '', f)))\n",
    "    #\n",
    "    hits = defaultdict(lambda : [])\n",
    "    ct = 0\n",
    "    fe_scores = {}\n",
    "    while len(pos_ixx) >= (1-coverage) * pos_tot_orig:\n",
    "        ct += 1\n",
    "        #\n",
    "        # GET BEST\n",
    "        assoc_scores = Counter()\n",
    "        for f in good_fragments:\n",
    "            table = ((pos_ct[f],pos_tot-pos_ct[f]),(neg_ct[f],neg_tot-neg_ct[f]))\n",
    "            try: fe_score = fe_scores[table]\n",
    "            except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n",
    "            assoc_scores[f] = (fe_score, string_props[f])\n",
    "        best, best_score = next((x for x in assoc_scores.most_common(1)),(None,None))\n",
    "        if best == None or best_score[0] < -np.log(5e-2):\n",
    "            break\n",
    "        # print([flist[k] for k,v in assoc_scores.most_common(10)])\n",
    "        #\n",
    "        # UPDATE\n",
    "        new_pos_ixx = []\n",
    "        for pos_v in pos_ixx:\n",
    "            if fragments[pos_v,best] > 0: hits[flist[best]].append(int(pos_v))\n",
    "            else: new_pos_ixx.append(pos_v)\n",
    "        #\n",
    "        pos_ixx = new_pos_ixx\n",
    "        pos_tot = len(pos_ixx)\n",
    "        pos_ct = fragments[pos_ixx].sum(0).A[0]\n",
    "        neg_ct = fragments[neg_ixx].sum(0).A[0]\n",
    "        #\n",
    "        good_fragments = np.where((pos_ct >= 1) & (pos_ct/pos_tot_orig >= min_trans) &\n",
    "                                    (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
    "        if verbose: print(ct, flist[best])\n",
    "    return hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505dfab-f473-4458-be42-830032aa01c5",
   "metadata": {},
   "source": [
    "### execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "200a83f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n",
      "/tmp/ipykernel_1464113/2656191247.py:59: RuntimeWarning: invalid value encountered in divide\n",
      "  (pos_ct/(pos_ct+neg_ct) >= min_backtrans))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hoch1243 2025-06-14 13:52:56.153782 655\n",
      "jeha1242 2025-06-14 13:53:04.254245 514\n",
      "jeju1234 2025-06-14 13:53:05.155374 963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1464113/2656191247.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cash1254 2025-06-14 13:53:06.930712 976\n",
      "anal1239 2025-06-14 13:53:08.441910 1189\n",
      "goro1270 2025-06-14 13:53:16.247044 1115\n",
      "even1259 2025-06-14 13:53:19.961585 1038\n",
      "apah1238 2025-06-14 13:53:22.238760 670\n",
      "arap1274 2025-06-14 13:53:32.069427 1056\n",
      "guri1247 2025-06-14 13:53:36.514740 724\n",
      "goem1240 2025-06-14 13:53:38.035993 989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1464113/2656191247.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beja1238 2025-06-14 13:53:51.538532 1317\n",
      "nngg1234 2025-06-14 13:54:05.580415 1054\n",
      "kaka1265 2025-06-14 13:54:05.957231 1804\n",
      "movi1243 2025-06-14 13:54:07.018497 1088\n",
      "cabe1245 2025-06-14 13:54:13.244728 813\n",
      "ngal1292 2025-06-14 13:54:17.488503 491\n",
      "dolg1241 2025-06-14 13:54:20.102199 1389\n",
      "nort2641 2025-06-14 13:54:25.044741 1105\n",
      "bain1259 2025-06-14 13:54:26.143536 1454\n",
      "kama1351 2025-06-14 13:54:26.857306 1421\n",
      "nort2875 2025-06-14 13:54:27.518135 926\n",
      "sanz1248 2025-06-14 13:54:37.551356 710\n",
      "pnar1238 2025-06-14 13:54:41.785540 1340\n",
      "orko1234 2025-06-14 13:54:43.576503 806\n",
      "komn1238 2025-06-14 13:54:44.708051 1503\n",
      "savo1255 2025-06-14 13:54:46.889652 564\n",
      "port1286 2025-06-14 13:54:49.256550 554\n",
      "sout2856 2025-06-14 13:54:50.185939 941\n",
      "teop1238 2025-06-14 13:54:52.503506 667\n",
      "kark1256 2025-06-14 13:54:52.774417 887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1464113/2656191247.py:40: RuntimeWarning: divide by zero encountered in log\n",
      "  except KeyError: fe_score = fe_scores[table] = -np.log(fisher_exact(table, alternative='greater')[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ligh1234 2025-06-14 13:54:52.871246 459\n",
      "ruul1235 2025-06-14 13:55:02.663705 1297\n",
      "vera1241 2025-06-14 13:55:04.163825 771\n",
      "texi1237 2025-06-14 13:55:06.298937 613\n",
      "tsim1256 2025-06-14 13:55:07.824600 669\n",
      "sumi1235 2025-06-14 13:55:08.657437 735\n",
      "svan1243 2025-06-14 13:55:09.550826 1224\n",
      "sadu1234 2025-06-14 13:55:10.984447 969\n",
      "toto1304 2025-06-14 13:55:12.019754 1050\n",
      "warl1254 2025-06-14 13:55:12.880729 442\n",
      "taba1259 2025-06-14 13:55:17.380680 583\n",
      "urum1249 2025-06-14 13:55:35.610131 976\n",
      "trin1278 2025-06-14 13:55:50.052480 1420\n"
     ]
    }
   ],
   "source": [
    "with Pool(8) as p:\n",
    "    p.starmap(create_alignments, map(lambda doc : (doc.strip('\\n'), 'doreco', 1), open('./doreco_english_doculects.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500cdbf-c833-4ac9-8abb-c95d1e6960ec",
   "metadata": {},
   "source": [
    "## AwesomeAlign model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ccbfd-2318-450d-80ca-93213b637eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate to generated/doreco_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09d009-89a5-46c2-93dd-ef61b06dfdc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1 create alignments\n",
    "from subprocess import call\n",
    "import os\n",
    "if not os.path.isdir('./generated/doreco_aa_output'):\n",
    "    os.mkdir('./generated/doreco_aa_output')\n",
    "for docname in map(lambda x : x.strip(), open('./doreco_english_doculects.txt')):\n",
    "    print(docname, datetime.now())\n",
    "    infile = './generated/doreco_bitexts/%s.txt' % docname\n",
    "    outfile = './generated/doreco_aa_output/%s.aligns' % docname\n",
    "    \n",
    "    call(['awesome-align', '--output_file=%s' % outfile, '--model_name_or_path=bert-base-multilingual-cased', '--data_file=%s' % infile,\n",
    "          '--extraction', 'softmax', '--batch_size', '32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "327329fd-6154-4877-9682-24aaccf2a84b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anal1239 2855 2855 2855 2025-06-14 13:52:33.498427\n",
      "661 0.09971338060039221\n",
      "apah1238 3268 3268 3268 2025-06-14 13:52:33.555471\n",
      "327 0.077524893314367\n",
      "arap1274 3230 3230 3230 2025-06-14 13:52:33.759950\n",
      "647 0.07553998832457677\n",
      "bain1259 2932 2932 2932 2025-06-14 13:52:33.830597\n",
      "644 0.09248886974005457\n",
      "beja1238 6330 6330 6330 2025-06-14 13:52:33.911204\n",
      "835 0.07755898198030838\n",
      "cabe1245 2058 2058 2058 2025-06-14 13:52:33.992223\n",
      "808 0.12986178077788493\n",
      "cash1254 1766 1766 1766 2025-06-14 13:52:34.148127\n",
      "508 0.07718018839258584\n",
      "dolg1241 2429 2429 2429 2025-06-14 13:52:34.214448\n",
      "1236 0.12395948249924782\n",
      "even1259 2311 2311 2311 2025-06-14 13:52:34.288019\n",
      "620 0.1000645577792124\n",
      "goem1240 2606 2606 2606 2025-06-14 13:52:34.362691\n",
      "1219 0.10547719996538894\n",
      "goro1270 4333 4333 4333 2025-06-14 13:52:34.448296\n",
      "712 0.07926965041193498\n",
      "guri1247 1326 1326 1326 2025-06-14 13:52:34.611473\n",
      "298 0.0837549184935357\n",
      "hoch1243 972 972 972 2025-06-14 13:52:34.643450\n",
      "389 0.0968384366442619\n",
      "jeha1242 1332 1332 1332 2025-06-14 13:52:34.674593\n",
      "330 0.09458297506448839\n",
      "jeju1234 1970 1970 1970 2025-06-14 13:52:34.712107\n",
      "484 0.10581547879317883\n",
      "kaka1265 4610 4610 4610 2025-06-14 13:52:34.801028\n",
      "1508 0.10569105691056911\n",
      "kama1351 9098 9098 9098 2025-06-14 13:52:35.073360\n",
      "2067 0.09756903469435922\n",
      "kark1256 1794 1794 1794 2025-06-14 13:52:35.186234\n",
      "514 0.08942240779401531\n",
      "komn1238 7521 7521 7521 2025-06-14 13:52:35.276496\n",
      "1389 0.09784446322907861\n",
      "ligh1234 1438 1438 1438 2025-06-14 13:52:35.367713\n",
      "266 0.06368206847019392\n",
      "movi1243 2194 2194 2194 2025-06-14 13:52:35.520905\n",
      "454 0.11638041527813381\n",
      "ngal1292 1679 1679 1679 2025-06-14 13:52:35.556258\n",
      "158 0.06267354224514082\n",
      "nngg1234 5921 5921 5921 2025-06-14 13:52:35.618910\n",
      "701 0.05870530106356252\n",
      "nort2641 834 834 834 2025-06-14 13:52:35.691760\n",
      "451 0.08538432411965165\n",
      "nort2875 2694 2694 2694 2025-06-14 13:52:35.736194\n",
      "432 0.09379070777247069\n",
      "orko1234 2939 2939 2939 2025-06-14 13:52:35.791428\n",
      "736 0.10263561567424348\n",
      "pnar1238 972 972 972 2025-06-14 13:52:35.953736\n",
      "405 0.06551277903591071\n",
      "port1286 1220 1220 1220 2025-06-14 13:52:35.992387\n",
      "395 0.09007981755986318\n",
      "ruul1235 2906 2906 2906 2025-06-14 13:52:36.035405\n",
      "471 0.08495670995670995\n",
      "sadu1234 1490 1490 1490 2025-06-14 13:52:36.077748\n",
      "628 0.1471070508315765\n",
      "sanz1248 460 460 460 2025-06-14 13:52:36.108535\n",
      "243 0.09371384496721943\n",
      "savo1255 780 780 780 2025-06-14 13:52:36.131253\n",
      "410 0.12991128010139416\n",
      "sout2856 1647 1647 1647 2025-06-14 13:52:36.176444\n",
      "764 0.08946135831381732\n",
      "sumi1235 2476 2476 2476 2025-06-14 13:52:36.233095\n",
      "483 0.09094332517416683\n",
      "svan1243 1299 1299 1299 2025-06-14 13:52:36.400598\n",
      "432 0.08947804473902236\n",
      "taba1259 632 632 632 2025-06-14 13:52:36.435151\n",
      "262 0.09205903021784961\n",
      "teop1238 1988 1988 1988 2025-06-14 13:52:36.466996\n",
      "499 0.10389339995835936\n",
      "texi1237 2824 2824 2824 2025-06-14 13:52:36.514222\n",
      "422 0.07335303320006953\n",
      "toto1304 3676 3676 3676 2025-06-14 13:52:36.564298\n",
      "500 0.09386146048432513\n",
      "trin1278 2007 2007 2007 2025-06-14 13:52:36.628096\n",
      "833 0.0894448620208311\n",
      "tsim1256 1591 1591 1591 2025-06-14 13:52:36.794515\n",
      "306 0.07617625093353249\n",
      "urum1249 2308 2308 2308 2025-06-14 13:52:36.851175\n",
      "918 0.10047061398708548\n",
      "vera1241 1932 1932 1932 2025-06-14 13:52:36.923641\n",
      "744 0.11365719523373052\n",
      "warl1254 2011 2011 2011 2025-06-14 13:52:36.973884\n",
      "431 0.08578821656050956\n"
     ]
    }
   ],
   "source": [
    "# 2 extract right lemmas\n",
    "for docname in map(lambda x : x.strip(), open('./doreco_english_doculects.txt')):\n",
    "    A = [[tuple(map(int, x.split('-'))) for x in l.strip().split()] for l in open('./generated/doreco_aa_output/%s.aligns' % docname)]\n",
    "    E,F = zip(*[([(w.split('/')[2], w.split('/')[3] in good_pos and w.split('/')[2] not in excluded_lemmas) for w in l.strip('\\n').split(' ||| ')[1].split() \n",
    "                  if w.count('/') >= 3],\n",
    "                  l.strip('\\n').split(' ||| ')[0].split())\n",
    "                 for l in open('./generated/doreco_bitexts/%s.spc' % (docname))])\n",
    "    print(docname, len(A), len(E), len(F), datetime.now())\n",
    "    hits= []\n",
    "    map_counts = defaultdict(lambda : Counter())\n",
    "    for i in range(len(A)):\n",
    "        maps = {}\n",
    "        #if i < 10: print(i, '|', len(E[i]), len(A[i]), '|', max(a[1] for a in A[i]), max(a[0] for a in A[i]), A[i])\n",
    "        for s,r in A[i]:\n",
    "            maps[r] = maps.get(r, []) + [s]\n",
    "        for j,(r,good) in enumerate(E[i]):\n",
    "            if not good: continue\n",
    "            hits.append(len(maps.get(j,[]))==2)\n",
    "            for s in maps.get(j,[]):\n",
    "                map_counts[r][F[i][s]] += 1\n",
    "\n",
    "    tes, te_words = defaultdict(lambda : defaultdict(lambda : [])), defaultdict(lambda : defaultdict(lambda : Counter()))\n",
    "    for i in range(len(A)):\n",
    "        maps = {}\n",
    "        for s,r in A[i]:\n",
    "            #print(E[i][r], F[i][s])\n",
    "            maps[r] = maps.get(r, []) + [s]\n",
    "        for j,(r,good) in enumerate(E[i]):\n",
    "            if not good: continue \n",
    "            s = max(maps.get(j,[]), key = lambda s : map_counts[r][F[i][s]], default = None)\n",
    "            if s != None:\n",
    "                tes[r][F[i][s]].append(i)\n",
    "                te_words[r][F[i][s]][F[i][s]] += 1\n",
    "            \n",
    "        #print('='*20)\n",
    "    print(sum(hits), np.mean(hits))\n",
    "    with open('./generated/doreco_aa_output/%s.json' % (docname),'w') as fout:\n",
    "        fout.write(json.dumps({'tes' : tes, 'te_words' : te_words}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c4f63-7b40-40f5-bec0-72d5debe87eb",
   "metadata": {},
   "source": [
    "## Eflomal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "54e1b317-0dd7-4c7f-9e8f-f419c4fffa44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anal1239 2025-06-14 13:18:02.827023\n",
      "apah1238 2025-06-14 13:18:03.402415\n",
      "arap1274 2025-06-14 13:18:03.832117\n",
      "bain1259 2025-06-14 13:18:04.498916\n",
      "beja1238 2025-06-14 13:18:05.132296\n",
      "cabe1245 2025-06-14 13:18:05.683570\n",
      "cash1254 2025-06-14 13:18:06.458669\n",
      "dolg1241 2025-06-14 13:18:07.139121\n",
      "even1259 2025-06-14 13:18:08.182965\n",
      "goem1240 2025-06-14 13:18:08.818280\n",
      "goro1270 2025-06-14 13:18:10.077142\n",
      "guri1247 2025-06-14 13:18:10.805136\n",
      "hoch1243 2025-06-14 13:18:11.378240\n",
      "jeha1242 2025-06-14 13:18:12.145142\n",
      "jeju1234 2025-06-14 13:18:12.669839\n",
      "kaka1265 2025-06-14 13:18:13.360523\n",
      "kama1351 2025-06-14 13:18:14.821348\n",
      "kark1256 2025-06-14 13:18:15.633825\n",
      "komn1238 2025-06-14 13:18:16.308339\n",
      "ligh1234 2025-06-14 13:18:17.054584\n",
      "movi1243 2025-06-14 13:18:17.582431\n",
      "ngal1292 2025-06-14 13:18:18.132541\n",
      "nngg1234 2025-06-14 13:18:18.496929\n",
      "nort2641 2025-06-14 13:18:19.162596\n",
      "nort2875 2025-06-14 13:18:20.193225\n",
      "orko1234 2025-06-14 13:18:20.681578\n",
      "pnar1238 2025-06-14 13:18:21.373883\n",
      "port1286 2025-06-14 13:18:23.200646\n",
      "ruul1235 2025-06-14 13:18:23.863177\n",
      "sadu1234 2025-06-14 13:18:24.419447\n",
      "sanz1248 2025-06-14 13:18:24.991874\n",
      "savo1255 2025-06-14 13:18:25.825937\n",
      "sout2856 2025-06-14 13:18:26.575922\n",
      "sumi1235 2025-06-14 13:18:27.917034\n",
      "svan1243 2025-06-14 13:18:28.520916\n",
      "taba1259 2025-06-14 13:18:29.463087\n",
      "teop1238 2025-06-14 13:18:30.122204\n",
      "texi1237 2025-06-14 13:18:30.640991\n",
      "toto1304 2025-06-14 13:18:31.129811\n",
      "trin1278 2025-06-14 13:18:31.623450\n",
      "tsim1256 2025-06-14 13:18:32.873214\n",
      "urum1249 2025-06-14 13:18:33.506940\n",
      "vera1241 2025-06-14 13:18:34.579412\n",
      "warl1254 2025-06-14 13:18:35.457703\n"
     ]
    }
   ],
   "source": [
    "# 1 create alignments\n",
    "from subprocess import call\n",
    "import os\n",
    "if not os.path.isdir('./generated/doreco_ef_output'):\n",
    "    os.mkdir('./generated/doreco_ef_output')\n",
    "for docname in map(lambda x : x.strip(), open('./doreco_english_doculects.txt')):\n",
    "    print(docname, datetime.now())\n",
    "    infile = './generated/doreco_bitexts/%s.txt' % docname\n",
    "    fwdfile = './generated/doreco_ef_output/%s.fwd' % docname\n",
    "    revfile = './generated/doreco_ef_output/%s.rev' % docname\n",
    "    \n",
    "    call(['eflomal-align', '-i', infile, '-f', fwdfile, '-r', revfile, '-m', '3'])\n",
    "    with open('./generated/doreco_ef_output/%s.gdfa' % docname,'w') as fout:\n",
    "        dump = open('dump.txt', 'w')\n",
    "        call(['./atools', '-i', fwdfile, '-j', revfile, '-c', 'grow-diag-final-and'], stdout=fout, stderr=dump)\n",
    "        dump.close()\n",
    "        os.remove('dump.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef960b00-bc8c-4caf-a881-650dbdd4cd6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anal1239 2855 2855 2855 2025-06-14 13:19:15.072404\n",
      "564 0.08508070598883694\n",
      "apah1238 3268 3268 3268 2025-06-14 13:19:15.127649\n",
      "204 0.04836415362731152\n",
      "arap1274 3230 3230 3230 2025-06-14 13:19:15.189377\n",
      "105 0.012259194395796848\n",
      "bain1259 2932 2932 2932 2025-06-14 13:19:15.259729\n",
      "269 0.038632773229929626\n",
      "beja1238 6330 6330 6330 2025-06-14 13:19:15.445002\n",
      "281 0.02610068734906186\n",
      "cabe1245 2058 2058 2058 2025-06-14 13:19:15.527914\n",
      "472 0.07585985213757634\n",
      "cash1254 1766 1766 1766 2025-06-14 13:19:15.586204\n",
      "330 0.05013673655423884\n",
      "dolg1241 2429 2429 2429 2025-06-14 13:19:15.654454\n",
      "538 0.05395647377394444\n",
      "even1259 2311 2311 2311 2025-06-14 13:19:15.840584\n",
      "279 0.045029051000645574\n",
      "goem1240 2606 2606 2606 2025-06-14 13:19:15.914285\n",
      "189 0.01635372501514234\n",
      "goro1270 4333 4333 4333 2025-06-14 13:19:16.002254\n",
      "305 0.03395680249387664\n",
      "guri1247 1326 1326 1326 2025-06-14 13:19:16.063398\n",
      "157 0.044125913434513775\n",
      "hoch1243 972 972 972 2025-06-14 13:19:16.097351\n",
      "232 0.05775454319143639\n",
      "jeha1242 1332 1332 1332 2025-06-14 13:19:16.130779\n",
      "187 0.05359701920321009\n",
      "jeju1234 1970 1970 1970 2025-06-14 13:19:16.167100\n",
      "260 0.05684302579798863\n",
      "kaka1265 4610 4610 4610 2025-06-14 13:19:16.365833\n",
      "758 0.05312587608634707\n",
      "kama1351 9098 9098 9098 2025-06-14 13:19:16.508695\n",
      "522 0.02464007552513571\n",
      "kark1256 1794 1794 1794 2025-06-14 13:19:16.734911\n",
      "296 0.05149617258176757\n",
      "komn1238 7521 7521 7521 2025-06-14 13:19:16.816813\n",
      "422 0.02972668357283742\n",
      "ligh1234 1438 1438 1438 2025-06-14 13:19:16.895483\n",
      "84 0.020110126885324396\n",
      "movi1243 2194 2194 2194 2025-06-14 13:19:16.932572\n",
      "102 0.026147141758523455\n",
      "ngal1292 1679 1679 1679 2025-06-14 13:19:16.962583\n",
      "31 0.012296707655692185\n",
      "nngg1234 5921 5921 5921 2025-06-14 13:19:17.127131\n",
      "218 0.0182564274348882\n",
      "nort2641 834 834 834 2025-06-14 13:19:17.197839\n",
      "204 0.03862173419159409\n",
      "nort2875 2694 2694 2694 2025-06-14 13:19:17.240673\n",
      "165 0.035822839774207554\n",
      "orko1234 2939 2939 2939 2025-06-14 13:19:17.294294\n",
      "312 0.04350857620973365\n",
      "pnar1238 972 972 972 2025-06-14 13:19:17.355208\n",
      "257 0.041572306696861856\n",
      "port1286 1220 1220 1220 2025-06-14 13:19:17.400993\n",
      "169 0.03854047890535918\n",
      "ruul1235 2906 2906 2906 2025-06-14 13:19:17.549663\n",
      "187 0.03373015873015873\n",
      "sadu1234 1490 1490 1490 2025-06-14 13:19:17.590745\n",
      "290 0.06793159990630124\n",
      "sanz1248 460 460 460 2025-06-14 13:19:17.619663\n",
      "233 0.08985730813729272\n",
      "savo1255 780 780 780 2025-06-14 13:19:17.641328\n",
      "228 0.07224334600760456\n",
      "sout2856 1647 1647 1647 2025-06-14 13:19:17.684529\n",
      "400 0.0468384074941452\n",
      "sumi1235 2476 2476 2476 2025-06-14 13:19:17.740585\n",
      "313 0.058934287328186785\n",
      "svan1243 1299 1299 1299 2025-06-14 13:19:17.783339\n",
      "251 0.0519884009942005\n",
      "taba1259 632 632 632 2025-06-14 13:19:17.936816\n",
      "166 0.05832747716092762\n",
      "teop1238 1988 1988 1988 2025-06-14 13:19:17.969758\n",
      "175 0.03643556110764106\n",
      "texi1237 2824 2824 2824 2025-06-14 13:19:18.023226\n",
      "105 0.018251347123240048\n",
      "toto1304 3676 3676 3676 2025-06-14 13:19:18.074611\n",
      "202 0.03792003003566736\n",
      "trin1278 2007 2007 2007 2025-06-14 13:19:18.139890\n",
      "284 0.030495006979491035\n",
      "tsim1256 1591 1591 1591 2025-06-14 13:19:18.203955\n",
      "167 0.041573313417973615\n",
      "urum1249 2308 2308 2308 2025-06-14 13:19:18.359590\n",
      "601 0.06577651307869103\n",
      "vera1241 1932 1932 1932 2025-06-14 13:19:18.425860\n",
      "351 0.053620531622364805\n",
      "warl1254 2011 2011 2011 2025-06-14 13:19:18.468390\n",
      "99 0.019705414012738853\n"
     ]
    }
   ],
   "source": [
    "# 2 extract right lemmas\n",
    "for docname in map(lambda x : x.strip(), open('./doreco_english_doculects.txt')):\n",
    "    A = [[tuple(map(int, x.split('-'))) for x in l.strip().split()] for l in open('./generated/doreco_ef_output/%s.gdfa' % docname)]\n",
    "    E,F = zip(*[([(w.split('/')[2], w.split('/')[3] in good_pos and w.split('/')[2] not in excluded_lemmas) for w in l.strip('\\n').split(' ||| ')[1].split() \n",
    "                  if w.count('/') >= 3],\n",
    "                  l.strip('\\n').split(' ||| ')[0].split())\n",
    "                 for l in open('./generated/doreco_bitexts/%s.spc' % (docname))])\n",
    "    print(docname, len(A), len(E), len(F), datetime.now())\n",
    "    hits= []\n",
    "    map_counts = defaultdict(lambda : Counter())\n",
    "    for i in range(len(A)):\n",
    "        maps = {}\n",
    "        #if i < 10: print(i, '|', len(E[i]), len(A[i]), '|', max(a[1] for a in A[i]), max(a[0] for a in A[i]), A[i])\n",
    "        for s,r in A[i]:\n",
    "            maps[r] = maps.get(r, []) + [s]\n",
    "        for j,(r,good) in enumerate(E[i]):\n",
    "            if not good: continue\n",
    "            hits.append(len(maps.get(j,[]))==2)\n",
    "            for s in maps.get(j,[]):\n",
    "                map_counts[r][F[i][s]] += 1\n",
    "\n",
    "    tes, te_words = defaultdict(lambda : defaultdict(lambda : [])), defaultdict(lambda : defaultdict(lambda : Counter()))\n",
    "    for i in range(len(A)):\n",
    "        maps = {}\n",
    "        for s,r in A[i]:\n",
    "            #print(E[i][r], F[i][s])\n",
    "            maps[r] = maps.get(r, []) + [s]\n",
    "        for j,(r,good) in enumerate(E[i]):\n",
    "            if not good: continue \n",
    "            s = max(maps.get(j,[]), key = lambda s : map_counts[r][F[i][s]], default = None)\n",
    "            if s != None:\n",
    "                tes[r][F[i][s]].append(i)\n",
    "                te_words[r][F[i][s]][F[i][s]] += 1\n",
    "            \n",
    "        #print('='*20)\n",
    "    print(sum(hits), np.mean(hits))\n",
    "    with open('./generated/doreco_ef_output/%s.json' % (docname),'w') as fout:\n",
    "        fout.write(json.dumps({'tes' : tes, 'te_words' : te_words}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
